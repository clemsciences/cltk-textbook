{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dominant-celebration",
   "metadata": {},
   "source": [
    "# <center>Commencer √† utiliser CLTK</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "muslim-citation",
   "metadata": {},
   "source": [
    "<center>Dr. W.J.B. Mattingly</center>\n",
    "\n",
    "<center>Smithsonian Data Science Lab and United States Holocaust Memorial Museum</center>\n",
    "\n",
    "<center>Avril 2022</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subject-france",
   "metadata": {},
   "source": [
    "## Abord√© dans ce chapitre"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "obvious-bones",
   "metadata": {},
   "source": [
    "1) <br>\n",
    "2) <br>\n",
    "3) <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addressed-giant",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd014184-30c1-44d4-a5dd-1b4ebbfd5bf0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3db8310f-1451-425d-a173-c01334f0e3a3",
   "metadata": {},
   "source": [
    "## R√©cup√©rer un texte"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53beeabe-c638-40a0-a3d3-7d9973768df6",
   "metadata": {},
   "source": [
    "Bien que CLTK permet de r√©cup√©rer des corpus de textes pour chaque langue, nous allons commencer par traiter des donn√©es stock√©es en local. Nous pouvons retrouver ce qu'on va faire dans <a href=\"https://github.com/cltk/cltk/blob/master/notebooks/CLTK%20Demonstration.ipynb\">carnet sur le d√©p√¥t de CLTK</a>. Dans le d√©p√¥t actuel, nous mettons √† disposition une collection de textes.\n",
    "Regardons un extrait de Tite-Live.\n",
    "\n",
    "Tout d'abord, nous avons besoin de r√©cup√©rer le texte en le stockant dans une variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "regional-place",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"texts/lat-livy.txt\") as f:\n",
    "    livy_full = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dependent-greeting",
   "metadata": {},
   "source": [
    "Excellent ! Maintenant, regardons ce que nous avons en main."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30d24be2-b1b5-4959-b087-5f94465d0931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text snippet: Iam primum omnium satis constat Troia capta in ceteros saevitum esse Troianos, duobus, Aeneae Antenorique, et vetusti iure hospitii et quia pacis reddendaeque Helenae semper auctores fuerant, omne ius\n",
      "Character count: 921462\n",
      "Approximate token count: 129799\n"
     ]
    }
   ],
   "source": [
    "print(\"Extrait du texte :\", livy_full[:200])\n",
    "print(\"Nombre de caract√®res :\", len(livy_full))\n",
    "print(\"Nombre approximatif de token :\", len(livy_full.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ethical-tourist",
   "metadata": {},
   "source": [
    "Nous utilisons l'expression \"nombre approximatif de tokens\" parce que les tokens sont consid√©r√©s comme quelque chose qui a une fonction syntaxique dans le texte. Cela signifie qu'un token n'est pas seulement un mot, mais aussi un signe de ponctuation par exemple. Nous utilisons le terme \"approximatif\" parce que la fonction `split()` transforme une cha√Æne de caract√®res en liste en consid√©rant par d√©faut le caract√®re espace. En d'autres termes, le nombre exact de tokens est plus √©lev√© puisque les signes de ponctuation peuvent √™tre coll√©s aux mots.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "previous-physiology",
   "metadata": {},
   "source": [
    "## Utiliser le tuyau (*pipeline*) de CLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "black-frame",
   "metadata": {},
   "source": [
    "CLTK est sp√©cialement con√ßu pour le traitement de langues naturelles appliqu√© aux langues antiques et m√©di√©vales. Pour utiliser au mieux cette biblioth√®que, nous avons d'abord besoin d'importer le tuyau mentionn√©."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "animal-governor",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cltk import NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ec29d8-8411-4596-8883-944446239c8b",
   "metadata": {},
   "source": [
    "Si le code ci-dessus a fonctionn√© sans g√©n√©rer d'erreur, alors cela signifie que nous avons correctement import√© la classe NLP depuis CLTK. Cela nous permet de cr√©er un tuyau de CLTK. Pour ce faire, nous avons cependant besoin de savoir la langue dans laquelle le texte a √©t√© √©crit. Tite-Live √©tait un auteur romain, la langue est le latin et son code est \"lat\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d752902d-c778-4a79-8adf-685782d1d3dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚Äéê§Ä CLTK version '1.0.25'.\n",
      "Pipeline for language 'Latin' (ISO: 'lat'): `LatinNormalizeProcess`, `LatinStanzaProcess`, `LatinEmbeddingsProcess`, `StopsProcess`, `LatinNERProcess`, `LatinLexiconProcess`.\n"
     ]
    }
   ],
   "source": [
    "# Charger le tuyau par d√©faut du latin\n",
    "cltk_nlp = NLP(language=\"lat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04ae7a9-47ec-48c7-857b-ce13a55853f2",
   "metadata": {},
   "source": [
    "It is Pythonic to create an NLP object with either the name \"nlp\" (spaCy syntax) or cltk_nlp (CLTK syntax). One reason for distinguishing between these two is that you may have two separate NLP pipelines in your workflow. It may help, therefore, to specify which nlp object is your cltk NLP pipeline. You can name this object whatever you like but it is best to stick to these conventions as it will make your code easier to understand.\n",
    "\n",
    "La sortie de cette cellule fournit des informations cl√©s sur notre tuyau. Ca inclut les diff√©rents tuyaux, ou processus appliqu√©s aux donn√©es d'entr√©e (le texte) :\n",
    "- LatinNormalizeProcess\n",
    "- LatinStanzaProcess\n",
    "- LatinEmbeddingsProcess\n",
    "- StopsProcess\n",
    "- LatinNERProcess\n",
    "- LatinLexiconProcess\n",
    "\n",
    "We will cover each of these in depth later in this notebook. For now, simply understand that as your text moves through the NLP class, it moves through a pipeline of different processes. The sequence here is important as some processes rely on post-processing from earlier pipes.\n",
    "Nous allons aborder l'ensemble des processus plus tard dans ce carnet. Pour l'instant, il faut comprendre que le texte qui est pass√© par une instance de la classe NLP passe √† travers un certain nombre de processus dans le tuyau. L'ordre est important √† respecter puisque des processus d√©pendent du r√©sultat des processus pr√©c√©dents.\n",
    "\n",
    "If we wish to remove a pipe from the pipeline, we can use .pipeline.process.pop(INSERT INDEX EHRER). Let's see what this looks like in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82606f42-e268-420b-a4b1-875f94d4e32b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<class 'cltk.alphabet.processes.LatinNormalizeProcess'>, <class 'cltk.dependency.processes.LatinStanzaProcess'>, <class 'cltk.embeddings.processes.LatinEmbeddingsProcess'>, <class 'cltk.stops.processes.StopsProcess'>, <class 'cltk.ner.processes.LatinNERProcess'>]\n"
     ]
    }
   ],
   "source": [
    "cltk_nlp.pipeline.processes.pop(-1)\n",
    "print(cltk_nlp.pipeline.processes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22fbe5c-532b-4998-be0c-01672368c92f",
   "metadata": {},
   "source": [
    "By using pop at -1, we are removing the final pipe. One reason for wishing to do this may be speed. The LatinLexiconProcess is one of the more time-consuming pipes in the pipeline and may not be necessary for your workflow which just needs to use the NER pipe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08b3a3f-f0dd-4723-aa5c-a90c029a406e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0bf8b3-f04f-49b1-b47e-b1068926c657",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50fda6e-53b3-4b91-a273-c172c27b5214",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d370d0-0c86-4b19-bb69-4acf180261b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "888a7f02-acca-47f4-bb8a-285084022f3c",
   "metadata": {},
   "source": [
    "## The CLTK Doc Object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8b8179-cc2d-4fb8-ad0d-824d8bfcce5d",
   "metadata": {},
   "source": [
    "Maintenant que nous avons mis en place notre tuyau, analysons un texte. Pour ce faire, nous allons cr√©er un objet CLTK Doc. Si vous √™tes d√©j√† famili√© avec spaCy ou d'autres biblioth√®ques de TAL, √ßa doit vous dire quelque chose. L'objet Doc contient les donn√©es du texte. Avant que nous examinions l'objet Doc, instancions en donc un. Tout d'abord, raccourcissons le texte de Tite-Live.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44905145-d258-4bef-a88f-126e1d8ad454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximate token count: 10905\n"
     ]
    }
   ],
   "source": [
    "livy = livy_full[:len(livy_full) // 12]\n",
    "print(\"Approximate token count:\", len(livy.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba335d5-b568-4e06-b1a7-864533789d6d",
   "metadata": {},
   "source": [
    "Now that we have shortened Livy, let's create the CLTK Doc object. To do this, we will run the CLTK NLP class object, call the analyze method and pass in one argument: the text which is livy. If it is your first time running this, you may be prompted to download the stanza models. Type \"Y\" to download them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac720b4d-6aa6-476a-bda6-f3c57238de96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLTK message: This part of the CLTK depends upon the Stanza NLP library.\n",
      "CLTK message: Allow download of Stanza models to ``C:\\Users\\wma22/stanza_resources/la/tokenize/ittb.pt``? [Y/n] \n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " y\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31d0b025fe6149f29cc9518a50664cdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.3.0.json:   0%|   ‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-20 08:53:02 INFO: Downloading these customized packages for language: la (Latin)...\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | ittb    |\n",
      "| pos       | ittb    |\n",
      "| lemma     | ittb    |\n",
      "| depparse  | ittb    |\n",
      "| pretrain  | ittb    |\n",
      "=======================\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd9a79a507c44afdb79fba28ccc78ddd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://huggingface.co/stanfordnlp/stanza-la/resolve/v1.3.0/models/tokenize/ittb.pt:   0%|        ‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a2d3e5f8ab648a8a7ede8999633edd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://huggingface.co/stanfordnlp/stanza-la/resolve/v1.3.0/models/pos/ittb.pt:   0%|          | 0‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afd78146315c4dceb327168660b1c390",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://huggingface.co/stanfordnlp/stanza-la/resolve/v1.3.0/models/lemma/ittb.pt:   0%|          |‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "630aef6d91ea44acb18419134869421a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://huggingface.co/stanfordnlp/stanza-la/resolve/v1.3.0/models/depparse/ittb.pt:   0%|        ‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a27f6e606914beca6cf8e3dfdb286c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://huggingface.co/stanfordnlp/stanza-la/resolve/v1.3.0/models/pretrain/ittb.pt:   0%|        ‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-20 08:53:59 INFO: Finished downloading models and saved to C:\\Users\\wma22\\stanza_resources.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This part of the CLTK depends upon models from the CLTK project.\n",
      "Do you want to download 'https://github.com/cltk/lat_models_cltk' to '~/cltk_data/lat'? [Y/n] \n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " y\n"
     ]
    }
   ],
   "source": [
    "cltk_doc = cltk_nlp.analyze(text=livy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53a62bd-4aa0-4f01-9d28-3f79722b756c",
   "metadata": {},
   "source": [
    "Now that all the models are downloaded, our pipeline should have completed its processing on the text. Let's start examining the  doc object a bit more closely. Let's first examining what type of object it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3872fa9e-8bc6-436b-a430-4dd79a6ae956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'cltk.core.data_types.Doc'>\n"
     ]
    }
   ],
   "source": [
    "print(type(cltk_doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87cffa52-ced7-43b8-b29c-b064bf64d9d7",
   "metadata": {},
   "source": [
    "Notice that it is a special class object that is related to the cltk, specifically a Doc object."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c355a91-3281-42e7-b9d6-ab3fe8522bd2",
   "metadata": {},
   "source": [
    "## Doc Object Accessors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab37dfa-b403-4cf8-8141-e867219b6dc2",
   "metadata": {},
   "source": [
    "The Doc object contains what the CLTK calls \"accessors\". If you are familiar with spaCy syntax, these function rather like spaCy attributes. They contain a specific piece of data. In some instances, this will be at the token level (e.g. token, lemmata, pos, etc.). In other cases, they occur at the sentence level (e.g. sentences, sentences_strings, sentences_tokens). This allows you to parse the Doc object in several different ways. Let's take a look at all the accessors that are available to us from the Latin pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "75228028-5221-4e89-9361-ebbf17b0c5a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_get_words_attribute\n",
      "embeddings\n",
      "embeddings_model\n",
      "language\n",
      "lemmata\n",
      "morphosyntactic_features\n",
      "normalized_text\n",
      "pipeline\n",
      "pos\n",
      "raw\n",
      "sentence_embeddings\n",
      "sentences\n",
      "sentences_strings\n",
      "sentences_tokens\n",
      "stanza_doc\n",
      "stems\n",
      "tokens\n",
      "tokens_stops_filtered\n",
      "words\n"
     ]
    }
   ],
   "source": [
    "accessors = ([x for x in dir(cltk_doc) if not x.startswith(\"__\")])\n",
    "for a in accessors:\n",
    "    print (a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3088fe57-397b-49e0-9cf5-ebd21712672c",
   "metadata": {},
   "source": [
    "Let's now examine some of these a bit more closely. Each will have a header so that you can use the navigation in the textbook (on the right of the screen) to navigate more easily."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647f2609-b99e-4411-a3da-45f2878d32cc",
   "metadata": {},
   "source": [
    "### Raw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a90828-c2af-4fd7-8723-94e015d045e5",
   "metadata": {},
   "source": [
    "The raw accessor is no different from the plain text object that we passed to the pipeline. It's index, therefore, functions just as the text input does. Let's take a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5936b2cc-7d04-4113-b77d-683ca7eb4cb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iam primum omnium sa\n"
     ]
    }
   ],
   "source": [
    "print (cltk_doc.raw[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7ddbd8-ec98-415e-b89b-3702751dfd12",
   "metadata": {},
   "source": [
    "### Token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c031317f-b98a-4cb0-8c98-106d641e806d",
   "metadata": {},
   "source": [
    "The token accessor, however, is fundamentally different. This accessor contains all sequential tokens in the text. Let's take a look at the first 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16806e4a-9b65-415a-b24c-dd994c2b2d50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Iam', 'primum', 'omnium', 'satis', 'constat', 'Troia', 'capta', 'in', 'ceteros', 'saevitum', 'esse', 'Troianos', ',', 'duobus', ',', 'Aeneae', 'Antenorique', ',', 'et', 'vetusti']\n"
     ]
    }
   ],
   "source": [
    "print(cltk_doc.tokens[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0450a881-8f2d-400a-bf04-b71c8a228d34",
   "metadata": {},
   "source": [
    "Notice how not only words are separated out in the output below, but also punctuation marks. This is what makes processing a text so powerful. We can analyze a text at the word level."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de18b233-b455-40c7-95e4-38e64cdcd5f6",
   "metadata": {},
   "source": [
    "### Lemmata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3a857f-dc27-4937-8e5c-9b889a1d1f0d",
   "metadata": {},
   "source": [
    "Like the token accessor, the lemmata accessor also functions at the token level. Unlike the token accessor, however, the lemmata contains the lemma forms of each token. Note \"capta\" above is now replaced with its lemma: \"capio\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d668ba6-099d-4089-86fb-f3a64589f45a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Iam', 'primus', 'omnis', 'satis', 'consto', 'Troia', 'capio', 'in', 'ceterus', 'saevitum', 'sum', 'Troianos', ',', 'duo', ',', 'Aeneae', 'Antenorique', ',', 'et', 'vetusti']\n"
     ]
    }
   ],
   "source": [
    "print(cltk_doc.lemmata[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee33325-62c7-4603-acd6-0936285daaba",
   "metadata": {},
   "source": [
    "### POS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09829517-4909-422d-9a6a-54685bdb2812",
   "metadata": {},
   "source": [
    "L'attribut `pos` fonctionne aussi au niveau du token. `pos` est l'acronyme de *part-of-speech* ou *pars oratori* en latin qui correspond au fran√ßais \"nature du mot\" ou \"nature grammaticale\". Cet attribut est tr√®s courant dans les biblioth√®ques de TAL.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "62d2e149-0a26-4237-96f5-8d2714f24b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ADV', 'ADJ', 'PRON', 'ADV', 'VERB', 'NOUN', 'VERB', 'ADP', 'PRON', 'VERB', 'AUX', 'NOUN', 'PUNCT', 'NUM', 'PUNCT', 'NOUN', 'VERB', 'PUNCT', 'CCONJ', 'NOUN']\n"
     ]
    }
   ],
   "source": [
    "print(cltk_doc.pos[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4277a39e-44fa-4d49-b993-a5bb57573e20",
   "metadata": {},
   "source": [
    "### Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17e1708-c5e2-48b0-84d0-845017ce3fd9",
   "metadata": {},
   "source": [
    "The words accessor may seem on the surface to resemble the token accessor, but it is a lot different. It contains all metadata about the word. It functions rather like spaCy's token attribute. Let's take a look at the seventh word in the text, \"capta\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c941998d-bef0-4945-a886-71ab500954d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word(index_char_start=None, index_char_stop=None, index_token=6, index_sentence=0, string='capta', pos=verb, lemma='capio', stem=None, scansion=None, xpos='L2|modM|tem4|grp1|casA|gen2', upos='VERB', dependency_relation='acl', governor=5, features={Aspect: [perfective], Case: [nominative], Degree: [positive], Gender: [feminine], Number: [singular], Tense: [past], VerbForm: [participle], Voice: [passive]}, category={F: [neg], N: [neg], V: [pos]}, stop=False, named_entity=False, syllables=None, phonetic_transcription=None, definition=None)\n"
     ]
    }
   ],
   "source": [
    "print (cltk_doc.words[6])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8126f703-4858-4d32-83c1-04be8556ebaf",
   "metadata": {},
   "source": [
    "Note that unlike the token accessor, the words accessor allows us to see all metadata relevant to this individual word. We can access each of these features as well. Let's say I was interested in knowing its part-of-speech. I can access that data like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "59026751-4589-4903-8808-670c967db580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "verb\n"
     ]
    }
   ],
   "source": [
    "print (cltk_doc.words[6].pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c60bf1-c6c6-4d23-a688-2ca9c77eef54",
   "metadata": {},
   "source": [
    "Maintenant que nous savons que c'est un verbe\n",
    "Now we know it is a verb. What if we wanted to know its voice? We could access its features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0df3406f-8567-4c70-b702-eb604b9dad91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{Aspect: [perfective], Case: [nominative], Degree: [positive], Gender: [feminine], Number: [singular], Tense: [past], VerbForm: [participle], Voice: [passive]}\n"
     ]
    }
   ],
   "source": [
    "print (cltk_doc.words[6].features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268c59cf-5632-407e-8d57-5b7dc4a04993",
   "metadata": {},
   "source": [
    "√Ä partir de l√†, nous pouvons naviguer\n",
    "And from here we can navigate this dictionary to the \"Voice\" key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5cf58243-8756-41da-ac15-d603c3e5777e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[passive]\n"
     ]
    }
   ],
   "source": [
    "print (cltk_doc.words[6].features[\"Voice\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99385ac8-b50c-4545-ac18-7ce421caa5af",
   "metadata": {},
   "source": [
    "And we can see that it is passive. We can access all these features equally easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "56a58626-95ce-4436-b655-ac0d4f3051eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number: [singular]\n",
      "Tense: [past]\n",
      "VerbForm: [participle]\n",
      "Voice: [passive]\n"
     ]
    }
   ],
   "source": [
    "print(\"Number:\", cltk_doc.words[6].features[\"Number\"])\n",
    "print(\"Tense:\", cltk_doc.words[6].features[\"Tense\"])\n",
    "print(\"VerbForm:\", cltk_doc.words[6].features[\"VerbForm\"]) \n",
    "print(\"Voice:\", cltk_doc.words[6].features[\"Voice\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02261d2c-175c-4236-bc1d-8dfde01b11fd",
   "metadata": {},
   "source": [
    "The words accessor is one of the more powerful aspects of the CLTK pipeline. I encourage you to spend a bit of time exploring what is available to you from the words accessor with your own text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57fa1819-e04f-49d0-81be-6ec9dced1fee",
   "metadata": {},
   "source": [
    "### Sentence Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f497b354-8430-47c1-af39-146fb962c16c",
   "metadata": {},
   "source": [
    "Contrairement aux attributs pr√©c√©dents, `sentence_tokens` nous permet d'analyser le texte au niveau de la phrase. Ainsi, le parsage phrase par phrase devient possible.\n",
    "Unlike the previous accessors, the sentence_tokens accessor allows us to analyze the Doc object at the sentencee level. This allows us to parse a text sentence-by-sentence which is not possible in Python. The split(\".\") approach separates a text at every \".\". This means that it will separate the text where a \".\" is used to denote an abbreviation. In Latin, as in English, this makes the approach impossible to use effectively. The CLTK pipeline, however, allows us to parse ancient and medieval languages effectively at the sentence level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7483fb7a-e51b-4f52-bc7e-5328c2dc9b74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Iam', 'primum', 'omnium', 'satis', 'constat', 'Troia', 'capta', 'in', 'ceteros', 'saevitum', 'esse', 'Troianos', ',', 'duobus', ',', 'Aeneae', 'Antenorique', ',', 'et', 'vetusti', 'iure', 'hospitii', 'et', 'quia', 'pacis', 'reddendaeque', 'Helenae', 'semper', 'auctores', 'fuerant', ',', 'omne', 'ius', 'belli', 'Achiuos', 'abstinuisse', ';'], ['casibus', 'deinde', 'variis', 'Antenorem', 'cum', 'multitudine', 'Enetum', ',', 'qui', 'seditione', 'ex', 'Paphlagonia', 'pulsi', 'et', 'sedes', 'et', 'ducem', 'rege', 'Pylaemene', 'ad', 'Troiam', 'amisso', 'quaerebant', ',', 'venisse', 'in', 'intimum', 'maris', 'Hadriatici', 'sinum', ',', 'Euganeisque', 'qui', 'inter', 'mare', 'Alpesque', 'incolebant', 'pulsis', 'Enetos', 'Troianosque', 'eas', 'tenuisse', 'terras', '.']]\n"
     ]
    }
   ],
   "source": [
    "print(cltk_doc.sentences_tokens[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floral-antique",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834f5782-9752-48f7-bba5-538a8f627954",
   "metadata": {},
   "source": [
    "Ce chapitre a pr√©sent√© les principales caract√©ristiques de la class NLP et nous avons vu comment construire le tuyau de traitement et comment y passer un texte √† travers lui. Dans le prochain chapitre, nous allons examiner plus particuli√®rement la reconnaissance d'entit√©s nomm√©es."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e76567c-bac9-48c8-ba9f-5acfa726df2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}